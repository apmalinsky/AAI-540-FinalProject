{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be811758-fd68-48a0-87f7-4dd9ca15ac4e",
   "metadata": {},
   "source": [
    "# Model Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c68ebf1-3c8e-4241-8692-5a620239fcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import boto3\n",
    "from threading import Thread\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role, session, Session, image_uris\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "from sagemaker.processing import ProcessingJob\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker.predictor import Predictor\n",
    "from time import gmtime, strftime, sleep\n",
    "from sagemaker.model_monitor import ModelQualityMonitor\n",
    "from sagemaker.model_monitor import EndpointInput\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from tqdm.notebook import tqdm\n",
    "from sagemaker.model_monitor import DefaultModelMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a643350-f928-47a8-b26d-ce993826fb44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:59:23.830460Z",
     "iopub.status.busy": "2025-10-15T23:59:23.830164Z",
     "iopub.status.idle": "2025-10-15T23:59:23.923914Z",
     "shell.execute_reply": "2025-10-15T23:59:23.922892Z",
     "shell.execute_reply.started": "2025-10-15T23:59:23.830436Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create CloudWatch client\n",
    "cw_client = boto3.Session().client(\"cloudwatch\")\n",
    "namespace = \"aws/sagemaker/Endpoints/model-metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a85b82-b396-4fe4-bbcf-3206672121b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:59:24.743933Z",
     "iopub.status.busy": "2025-10-15T23:59:24.743629Z",
     "iopub.status.idle": "2025-10-15T23:59:25.385042Z",
     "shell.execute_reply": "2025-10-15T23:59:25.384273Z",
     "shell.execute_reply.started": "2025-10-15T23:59:24.743911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: sagemaker-us-east-1-381492037991\n",
      "Capture path: s3://sagemaker-us-east-1-381492037991/sagemaker/FoodLens-ModelQualityMonitor-2025-10-15-23-59-25/datacapture\n",
      "Ground truth path: s3://sagemaker-us-east-1-381492037991/sagemaker/FoodLens-ModelQualityMonitor-2025-10-15-23-59-25/ground_truth_data/2025-10-15-23-59-25\n",
      "Report path: s3://sagemaker-us-east-1-381492037991/sagemaker/FoodLens-ModelQualityMonitor-2025-10-15-23-59-25/reports\n"
     ]
    }
   ],
   "source": [
    "# Setup boto and sagemaker session\n",
    "sagemaker_session = Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# Setup S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(\"Bucket:\", bucket)\n",
    "prefix = f\"sagemaker/FoodLens-ModelQualityMonitor-{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "\n",
    "# S3 prefixes\n",
    "data_capture_prefix = f\"{prefix}/datacapture\"\n",
    "s3_capture_upload_path = f\"s3://{bucket}/{data_capture_prefix}\"\n",
    "\n",
    "ground_truth_upload_path = (\n",
    "    f\"s3://{bucket}/{prefix}/ground_truth_data/{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    ")\n",
    "\n",
    "reports_prefix = f\"{prefix}/reports\"\n",
    "s3_report_path = f\"s3://{bucket}/{reports_prefix}\"\n",
    "\n",
    "print(f\"Capture path: {s3_capture_upload_path}\")\n",
    "print(f\"Ground truth path: {ground_truth_upload_path}\")\n",
    "print(f\"Report path: {s3_report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75defba-e5ff-4fb6-8d5a-31febc6b0d5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:59:26.129192Z",
     "iopub.status.busy": "2025-10-15T23:59:26.128736Z",
     "iopub.status.idle": "2025-10-15T23:59:26.244236Z",
     "shell.execute_reply": "2025-10-15T23:59:26.243240Z",
     "shell.execute_reply.started": "2025-10-15T23:59:26.129085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n",
      "{'UserId': 'AROAVRUVSRFTVVZM55U34:SageMaker', 'Account': '381492037991', 'Arn': 'arn:aws:sts::381492037991:assumed-role/LabRole/SageMaker', 'ResponseMetadata': {'RequestId': '2b297211-b372-4e2e-9071-a54207933f6c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '2b297211-b372-4e2e-9071-a54207933f6c', 'x-amz-sts-extended-request-id': 'MTp1cy1lYXN0LTE6MTc2MDU3Mjc2NjIyNjpHOlAzR05QbTNp', 'content-type': 'text/xml', 'content-length': '432', 'date': 'Wed, 15 Oct 2025 23:59:26 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "print(boto3.Session().region_name)\n",
    "print(boto3.client(\"sts\").get_caller_identity())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d279326a-9865-4617-afe8-d2544805be20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T23:59:27.745068Z",
     "iopub.status.busy": "2025-10-15T23:59:27.744778Z",
     "iopub.status.idle": "2025-10-15T23:59:27.887144Z",
     "shell.execute_reply": "2025-10-15T23:59:27.886227Z",
     "shell.execute_reply.started": "2025-10-15T23:59:27.745044Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm = boto3.client(\"sagemaker\", region_name=\"us-east-1\")\n",
    "\n",
    "for job in sm.list_training_jobs(NameContains=\"xgb-nutriscore\")[\"TrainingJobSummaries\"]:\n",
    "    print(job[\"TrainingJobName\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ae329f-4cb4-4868-9de6-6730d47a40ff",
   "metadata": {},
   "source": [
    "## Deploy Pre-Trained Model to Live Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "983824bd-f86f-46b5-a9e2-bf155eeead03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:32:52.339637Z",
     "iopub.status.busy": "2025-10-15T07:32:52.339353Z",
     "iopub.status.idle": "2025-10-15T07:32:52.522221Z",
     "shell.execute_reply": "2025-10-15T07:32:52.521570Z",
     "shell.execute_reply.started": "2025-10-15T07:32:52.339617Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the sagemaker client\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# Specify model\n",
    "image_uri = image_uris.retrieve(framework=\"xgboost\", region=region, version=\"1.7-1\")\n",
    "instance_type = 'ml.m5.xlarge'\n",
    "model_name = 'nutrition-score-xgb-2025-10-15-05-02-13' # get from notebook 04\n",
    "response = sagemaker_client.describe_model(ModelName=model_name)\n",
    "model_url = response['PrimaryContainer']['ModelDataUrl']\n",
    "model = Model(image_uri=image_uri, model_data=model_url, role=role, sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e9c518b-6006-4725-b4c5-b52e89a670ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:33:09.121385Z",
     "iopub.status.busy": "2025-10-15T07:33:09.121122Z",
     "iopub.status.idle": "2025-10-15T07:36:41.460165Z",
     "shell.execute_reply": "2025-10-15T07:36:41.459406Z",
     "shell.execute_reply.started": "2025-10-15T07:33:09.121363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointName:  xgb-nutriscore-model-quality-monitor-2025-10-15-07-33-09\n",
      "Deploying endpoint....\n",
      "------!\n",
      "Endpoint 'xgb-nutriscore-model-quality-monitor-2025-10-15-07-33-09' in Service.\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = f\"xgb-nutriscore-model-quality-monitor-{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "print(\"EndpointName: \", endpoint_name)\n",
    "\n",
    "# Enable data capture\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True, sampling_percentage=100, destination_s3_uri=s3_capture_upload_path\n",
    ")\n",
    "\n",
    "# Deploy the model and wait for it to be in service\n",
    "print(\"Deploying endpoint....\")\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    data_capture_config=data_capture_config,\n",
    ")\n",
    "\n",
    "print(f\"\\nEndpoint '{endpoint_name}' in Service.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e2b84a6-7d74-497a-b687-cc2e4c57d92c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:36:56.018667Z",
     "iopub.status.busy": "2025-10-15T07:36:56.018395Z",
     "iopub.status.idle": "2025-10-15T07:36:56.022019Z",
     "shell.execute_reply": "2025-10-15T07:36:56.021029Z",
     "shell.execute_reply.started": "2025-10-15T07:36:56.018646Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create predictor object\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name, \n",
    "    sagemaker_session=sagemaker_session, \n",
    "    serializer=CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2002d11a-3fc1-4ce3-9221-4fd487ac60b4",
   "metadata": {},
   "source": [
    "## Setup Infrastructure Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33fd9809-5b93-45ba-9bfd-4c1d934f94ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:36:58.425947Z",
     "iopub.status.busy": "2025-10-15T07:36:58.425669Z",
     "iopub.status.idle": "2025-10-15T07:36:58.710258Z",
     "shell.execute_reply": "2025-10-15T07:36:58.709129Z",
     "shell.execute_reply.started": "2025-10-15T07:36:58.425926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭─────────────────────────────── </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">11</span>                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span>]                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 # Create the alarm</span>                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>11 <span style=\"font-weight: bold; text-decoration: underline\">cw_client.put_metric_alarm(</span>                                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold; text-decoration: underline\">## fill here</span>                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span><span style=\"font-weight: bold; text-decoration: underline\">)</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.12/site-packages/botocore/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">client.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">569</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_api_call</span>                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 566 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>py_operation_name<span style=\"color: #808000; text-decoration-color: #808000\">}() only accepts keyword arguments.\"</span>              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 567 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 568 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The \"self\" in this scope is referring to the BaseClient.</span>                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 569 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">._make_api_call(operation_name, kwargs)</span>                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 570 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 571 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>_api_call.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(py_operation_name)                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 572 </span>                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.12/site-packages/botocore/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">client.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">980</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_make_api_call</span>                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 977 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Pass arbitrary endpoint info with the Request</span>                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 978 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># for use during construction.</span>                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 979 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>request_context[<span style=\"color: #808000; text-decoration-color: #808000\">'endpoint_properties'</span>] = properties                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 980 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>request_dict = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._convert_to_request_dict(                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 981 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>api_params=api_params,                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 982 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>operation_model=operation_model,                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 983 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>endpoint_url=endpoint_url,                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.12/site-packages/botocore/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">client.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1047</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_convert_to_request_dict</span>      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1044 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>headers=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>,                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1045 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>set_user_agent_header=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>,                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1046 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>):                                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1047 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>request_dict = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">._serializer.serialize_to_request(</span>                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1048 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold; text-decoration: underline\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">api_params, operation_model</span>                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1049 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold; text-decoration: underline\">│   │   </span><span style=\"font-weight: bold; text-decoration: underline\">)</span>                                                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1050 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._client_config.inject_host_prefix:                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.12/site-packages/botocore/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">validate.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">381</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">serialize_to_request</span>         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">378 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>parameters, operation_model.input_shape                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">379 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">380 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> report.has_errors():                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>381 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">raise</span><span style=\"font-weight: bold; text-decoration: underline\"> ParamValidationError(report=report.generate_report())</span>                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">382 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._serializer.serialize_to_request(                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">383 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>parameters, operation_model                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">384 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ParamValidationError: </span>Parameter validation failed:\n",
       "Missing required parameter in input: <span style=\"color: #008700; text-decoration-color: #008700\">\"AlarmName\"</span>\n",
       "Missing required parameter in input: <span style=\"color: #008700; text-decoration-color: #008700\">\"EvaluationPeriods\"</span>\n",
       "Missing required parameter in input: <span style=\"color: #008700; text-decoration-color: #008700\">\"ComparisonOperator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0m╭─\u001b[0m\u001b[38;2;255;0;0m──────────────────────────────\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0m───────────────────────────────\u001b[0m\u001b[38;2;255;0;0m─╮\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m11\u001b[0m                                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 8 \u001b[0m]                                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 9 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m# Create the alarm\u001b[0m                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m11 \u001b[1;4mcw_client.put_metric_alarm(\u001b[0m                                                                 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[1;2;4m## fill here\u001b[0m                                                                                \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m13 \u001b[0m\u001b[1;4m)\u001b[0m                                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m14 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.12/site-packages/botocore/\u001b[0m\u001b[1;33mclient.py\u001b[0m:\u001b[94m569\u001b[0m in \u001b[92m_api_call\u001b[0m                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 566 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mpy_operation_name\u001b[33m}\u001b[0m\u001b[33m() only accepts keyword arguments.\u001b[0m\u001b[33m\"\u001b[0m              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 567 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 568 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m 569 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4;96mself\u001b[0m\u001b[1;4m._make_api_call(operation_name, kwargs)\u001b[0m                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 570 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 571 \u001b[0m\u001b[2m│   │   \u001b[0m_api_call.\u001b[91m__name__\u001b[0m = \u001b[96mstr\u001b[0m(py_operation_name)                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 572 \u001b[0m                                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.12/site-packages/botocore/\u001b[0m\u001b[1;33mclient.py\u001b[0m:\u001b[94m980\u001b[0m in \u001b[92m_make_api_call\u001b[0m                 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 977 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Pass arbitrary endpoint info with the Request\u001b[0m                               \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 978 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# for use during construction.\u001b[0m                                                \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 979 \u001b[0m\u001b[2m│   │   │   \u001b[0mrequest_context[\u001b[33m'\u001b[0m\u001b[33mendpoint_properties\u001b[0m\u001b[33m'\u001b[0m] = properties                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m 980 \u001b[2m│   │   \u001b[0mrequest_dict = \u001b[96mself\u001b[0m._convert_to_request_dict(                                     \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 981 \u001b[0m\u001b[2m│   │   │   \u001b[0mapi_params=api_params,                                                        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 982 \u001b[0m\u001b[2m│   │   │   \u001b[0moperation_model=operation_model,                                              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 983 \u001b[0m\u001b[2m│   │   │   \u001b[0mendpoint_url=endpoint_url,                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.12/site-packages/botocore/\u001b[0m\u001b[1;33mclient.py\u001b[0m:\u001b[94m1047\u001b[0m in \u001b[92m_convert_to_request_dict\u001b[0m      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1044 \u001b[0m\u001b[2m│   │   \u001b[0mheaders=\u001b[94mNone\u001b[0m,                                                                     \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1045 \u001b[0m\u001b[2m│   │   \u001b[0mset_user_agent_header=\u001b[94mTrue\u001b[0m,                                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1046 \u001b[0m\u001b[2m│   \u001b[0m):                                                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m1047 \u001b[2m│   │   \u001b[0mrequest_dict = \u001b[1;4;96mself\u001b[0m\u001b[1;4m._serializer.serialize_to_request(\u001b[0m                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1048 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4mapi_params, operation_model\u001b[0m                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1049 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4m)\u001b[0m                                                                                 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1050 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m._client_config.inject_host_prefix:                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.12/site-packages/botocore/\u001b[0m\u001b[1;33mvalidate.py\u001b[0m:\u001b[94m381\u001b[0m in \u001b[92mserialize_to_request\u001b[0m         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m378 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mparameters, operation_model.input_shape                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m379 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m380 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m report.has_errors():                                                        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m381 \u001b[2m│   │   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m ParamValidationError(report=report.generate_report())\u001b[0m                \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m382 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._serializer.serialize_to_request(                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m383 \u001b[0m\u001b[2m│   │   │   \u001b[0mparameters, operation_model                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m384 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mParamValidationError: \u001b[0mParameter validation failed:\n",
       "Missing required parameter in input: \u001b[38;2;0;135;0m\"AlarmName\"\u001b[0m\n",
       "Missing required parameter in input: \u001b[38;2;0;135;0m\"EvaluationPeriods\"\u001b[0m\n",
       "Missing required parameter in input: \u001b[38;2;0;135;0m\"ComparisonOperator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a CloudWatch alarm for model latency\n",
    "alarm_name = \"NUTRISCORE_MODEL_LATENCY_HIGH\"\n",
    "alarm_desc = \"Trigger an alarm when the average model latency exceeds 200ms.\"\n",
    "infrastructure_metric_name = 'ModelLatency'\n",
    "cw_latency_dimensions = [\n",
    "    {\"Name\": \"Endpoint\", \"Value\": endpoint_name},\n",
    "    {\"Name\": \"MonitoringSchedule\", \"Value\": 'AllTraffic'},\n",
    "]\n",
    "\n",
    "# Create the alarm\n",
    "cw_client.put_metric_alarm(\n",
    "## fill here\n",
    ")\n",
    "\n",
    "print(f\"CloudWatch alarm '{alarm_name}' for infrastructure latency has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4336829-24bd-42c9-b6c3-698770ad4e23",
   "metadata": {},
   "source": [
    "## Setup Data Quality Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b561f37-0a0f-40bb-b609-b58be901e649",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T01:28:44.548175Z",
     "iopub.status.busy": "2025-10-16T01:28:44.547899Z",
     "iopub.status.idle": "2025-10-16T01:28:46.962048Z",
     "shell.execute_reply": "2025-10-16T01:28:46.959540Z",
     "shell.execute_reply.started": "2025-10-16T01:28:44.548155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape after cleanup: (39955, 23)\n",
      "Columns: ['code', 'product_name', 'nova_group', 'additives_n', 'ingredients_n', 'nutriscore_score', 'energy_100g', 'salt_100g', 'carbohydrates_100g', 'cholesterol_100g', 'sodium_100g', 'fiber_100g', 'fruits_vegetables_legumes_estimate_from_ingredients_100g', 'sugars_100g', 'saturated_fat_100g', 'trans_fat_100g', 'fat_100g', 'proteins_100g', 'fruits_vegetables_nuts_estimate_from_ingredients_100g', 'energy_kcal_100g', 'nova_group_100g', 'nutrition_score_fr_100g', 'energy_kj_100g']\n",
      "Final ready CSV uploaded to s3://sagemaker-us-east-1-381492037991/nutriscore-prediction-xgboost/train/train_scaled_ready_final.csv\n"
     ]
    }
   ],
   "source": [
    "#Ignore this section, but use already cleaned csv train_scaled_ready_final.csv\n",
    "#the data cleanup step removes empty and non-numeric columns and adds headers \n",
    "#This is make csv compatible with SageMaker’s data requirements.  \n",
    "\n",
    "import io\n",
    "\n",
    "# Define S3 keys (paths inside your bucket)\n",
    "key = \"nutriscore-prediction-xgboost/train/train_scaled_ready2.csv\"\n",
    "final_ready_key = \"nutriscore-prediction-xgboost/train/train_scaled_ready_final.csv\"\n",
    "\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "# Download, clean, and upload ---\n",
    "obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "\n",
    "# Drop empty columns and keep only numeric columns\n",
    "df_clean = (\n",
    "    df.dropna(axis=1, how=\"all\")\n",
    "      .select_dtypes(include=[\"number\"])\n",
    ")\n",
    "\n",
    "print(f\"Final shape after cleanup: {df_clean.shape}\")\n",
    "print(f\"Columns: {list(df_clean.columns)}\")\n",
    "\n",
    "# Save locally and upload back to S3\n",
    "local_path = \"/tmp/train_scaled_ready_final.csv\"\n",
    "df_clean.to_csv(local_path, index=False)\n",
    "s3.upload_file(local_path, bucket, final_ready_key)\n",
    "\n",
    "print(f\"Final ready CSV uploaded to s3://{bucket}/{final_ready_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d6e4864-3278-43e8-b521-59b46a96e22e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T01:00:22.820856Z",
     "iopub.status.busy": "2025-10-16T01:00:22.820596Z",
     "iopub.status.idle": "2025-10-16T01:06:17.342457Z",
     "shell.execute_reply": "2025-10-16T01:06:17.341680Z",
     "shell.execute_reply.started": "2025-10-16T01:00:22.820836Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating processing-job with name nutriscore-data-quality-baseline-job-2025-10-16-01-00-22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Data Quality baseline suggestion job...\n",
      ".................\u001b[34m2025-10-16 01:03:10.458083: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:10.458112: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:12.034444: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:12.034476: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:12.034497: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-198-177.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:12.034802: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:13,628 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:381492037991:processing-job/nutriscore-data-quality-baseline-job-2025-10-16-01-00-22', 'ProcessingJobName': 'nutriscore-data-quality-baseline-job-2025-10-16-01-00-22', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-381492037991/nutriscore-prediction-xgboost/train/train_scaled_ready_final.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-381492037991/nutriscore-prediction-xgboost/data-quality-reports', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::381492037991:role/LabRole', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:13,628 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:13,629 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:13,629 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:13,629 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:13,629 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:13,691 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:13,692 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:13,692 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:13,701 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:13,701 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:13,701 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,174 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.198.177\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-\u001b[0m\n",
      "\u001b[34myarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,184 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,188 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-8fbae3d6-cc79-4414-880f-209f7f17adb1\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,741 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,755 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,756 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,759 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,764 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,764 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,764 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,764 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,794 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,804 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,805 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,809 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,812 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Oct 16 01:03:14\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,813 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,813 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,814 INFO util.GSet: 2.0% max memory 3.1 GB = 63.6 MB\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,814 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,886 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,889 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,889 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,890 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,890 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,890 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,890 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,890 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,890 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,890 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,890 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,890 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,915 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,915 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,915 INFO util.GSet: 1.0% max memory 3.1 GB = 31.8 MB\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,915 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,917 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,917 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,917 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,917 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,921 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,925 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,925 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,925 INFO util.GSet: 0.25% max memory 3.1 GB = 8.0 MB\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,925 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,931 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,931 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,931 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,934 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,935 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,936 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,936 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,937 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 977.0 KB\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,937 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,956 INFO namenode.FSImage: Allocated new BlockPoolId: BP-458530468-10.2.198.177-1760576594950\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,969 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:14,976 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:15,053 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:15,065 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:15,068 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.198.177\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:15,080 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:17,137 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:17,138 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:19,206 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:19,207 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:21,298 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:21,299 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:23,393 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:23,393 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:25,493 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:25,494 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:35,502 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:37,045 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:37,499 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:37,538 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:37,550 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,118 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,141 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,141 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,142 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,142 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,171 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11507, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,185 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,186 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,233 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,233 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,234 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,234 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,234 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,546 INFO util.Utils: Successfully started service 'sparkDriver' on port 32853.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,573 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,604 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,621 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,622 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,653 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,674 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-405a0ea0-f74b-4250-8f07-0ec6f22a6fe2\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,689 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,724 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:38,755 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.2.198.177:32853/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1760576618113\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:39,235 INFO client.RMProxy: Connecting to ResourceManager at /10.2.198.177:8032\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:39,877 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:39,878 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:39,886 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15692 MB per container)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:39,887 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:39,887 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:39,887 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:39,895 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:39,970 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:42,189 INFO yarn.Client: Uploading resource file:/tmp/spark-7f929bf6-0c84-4b63-8a65-27638f548cf8/__spark_libs__3228971397073588340.zip -> hdfs://10.2.198.177/user/root/.sparkStaging/application_1760576600636_0001/__spark_libs__3228971397073588340.zip\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:43,341 INFO yarn.Client: Uploading resource file:/tmp/spark-7f929bf6-0c84-4b63-8a65-27638f548cf8/__spark_conf__2289994468313934125.zip -> hdfs://10.2.198.177/user/root/.sparkStaging/application_1760576600636_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:43,398 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:43,398 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:43,398 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:43,398 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:43,398 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:43,431 INFO yarn.Client: Submitting application application_1760576600636_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:43,616 INFO impl.YarnClientImpl: Submitted application application_1760576600636_0001\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:44,649 INFO yarn.Client: Application report for application_1760576600636_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:44,653 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Thu Oct 16 01:03:44 +0000 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1760576623525\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1760576600636_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:45,656 INFO yarn.Client: Application report for application_1760576600636_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:46,659 INFO yarn.Client: Application report for application_1760576600636_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:47,663 INFO yarn.Client: Application report for application_1760576600636_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:48,667 INFO yarn.Client: Application report for application_1760576600636_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:49,057 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1760576600636_0001), /proxy/application_1760576600636_0001\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:49,670 INFO yarn.Client: Application report for application_1760576600636_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:49,671 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.198.177\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1760576623525\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1760576600636_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:49,672 INFO cluster.YarnClientSchedulerBackend: Application application_1760576600636_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:49,680 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38433.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:49,680 INFO netty.NettyBlockTransferService: Server created on 10.2.198.177:38433\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:49,681 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:49,687 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.2.198.177, 38433, None)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:49,690 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.2.198.177:38433 with 1458.6 MiB RAM, BlockManagerId(driver, 10.2.198.177, 38433, None)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:49,693 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.2.198.177, 38433, None)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:49,694 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.2.198.177, 38433, None)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:49,793 INFO util.log: Logging initialized @14106ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:50,328 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:53,418 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.198.177:37664) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:03:53,631 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:37933 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 37933, None)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:09,170 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:09,382 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:09,438 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:09,443 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:10,463 INFO datasources.InMemoryFileIndex: It took 37 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:10,622 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:10,919 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.3 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:10,922 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.198.177:38433 (size: 39.3 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:10,927 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,298 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,301 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,307 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,364 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,383 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,383 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,384 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,385 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,394 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,455 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,460 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,461 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.2.198.177:38433 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,462 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,479 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,480 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,524 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4637 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:11,745 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:37933 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:12,444 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:37933 (size: 39.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:12,770 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1262 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:12,772 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:12,777 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.349 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:12,781 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:12,781 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:12,783 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.418893 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:12,945 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.2.198.177:38433 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:12,952 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:37933 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,107 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,109 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,111 INFO datasources.FileSourceStrategy: Output Data Schema: struct<code: string, product_name: string, nova_group: string, additives_n: string, ingredients_n: string ... 21 more fields>\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,336 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,351 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,352 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.2.198.177:38433 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,356 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,370 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 7159064 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,417 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,418 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,418 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,419 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,422 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,427 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,481 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 19.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,482 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,483 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.2.198.177:38433 (size: 8.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,484 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,485 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,485 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,489 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:15,549 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:37933 (size: 8.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:16,365 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:37933 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:17,351 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:37933 (size: 4.4 MiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:17,506 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2020 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:17,507 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:17,512 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 2.081 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:17,513 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:17,513 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:17,517 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 2.100023 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:17,863 INFO codegen.CodeGenerator: Code generated in 263.976512 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:18,450 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:18,592 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:18,597 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:18,597 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:18,598 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:18,600 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:18,602 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:18,631 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 116.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:18,633 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:18,634 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.2.198.177:38433 (size: 35.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:18,635 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:18,637 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:18,637 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:18,645 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:18,680 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:37933 (size: 35.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,102 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1459 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,102 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,104 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.498 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,105 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,105 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,106 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,106 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,189 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,191 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,191 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,191 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,191 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,192 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,204 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 169.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,206 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 46.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,206 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.2.198.177:38433 (size: 46.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,207 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,207 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,207 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,209 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,223 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:37933 (size: 46.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,264 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.2.198.177:37664\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,626 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 417 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,627 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,628 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.430 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,630 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,630 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,631 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.441827 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:20,725 INFO codegen.CodeGenerator: Code generated in 74.426477 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:21,065 INFO codegen.CodeGenerator: Code generated in 36.138813 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:21,145 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:21,147 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:21,147 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:21,147 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:21,149 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:21,152 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:21,182 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 40.6 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:21,184 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:21,185 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.2.198.177:38433 (size: 17.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:21,192 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:21,192 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:21,195 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:21,197 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:21,212 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:37933 (size: 17.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:22,538 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 1342 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:22,538 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:22,539 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 1.386 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:22,540 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:22,541 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:22,542 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 1.396715 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:22,958 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.2.198.177:38433 in memory (size: 8.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:22,962 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:37933 in memory (size: 8.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:22,985 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.2.198.177:38433 in memory (size: 46.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:22,986 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:37933 in memory (size: 46.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,006 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:37933 in memory (size: 35.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,009 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.2.198.177:38433 in memory (size: 35.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,018 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:37933 in memory (size: 17.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,020 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.2.198.177:38433 in memory (size: 17.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,136 INFO codegen.CodeGenerator: Code generated in 110.933872 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,144 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,144 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,144 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,145 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,145 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,146 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,152 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 76.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,154 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 24.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,155 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.2.198.177:38433 (size: 24.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,156 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,156 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,157 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,158 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,180 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:37933 (size: 24.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,490 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 331 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,490 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,491 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.343 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,492 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,492 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,493 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,493 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,699 INFO codegen.CodeGenerator: Code generated in 97.295837 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,716 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,718 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,719 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,719 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,719 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,724 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,728 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,732 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,734 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.2.198.177:38433 (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,735 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,736 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,736 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,738 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,756 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:37933 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,764 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.2.198.177:37664\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,908 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 170 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,908 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,909 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.182 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,909 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,909 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,911 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.193661 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:23,989 INFO codegen.CodeGenerator: Code generated in 51.639527 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:24,145 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:24,149 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:24,150 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:24,150 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:24,150 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:24,151 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:24,154 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:24,164 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 32.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:24,166 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:24,167 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.2.198.177:38433 (size: 14.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:24,167 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:24,168 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:24,168 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:24,170 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:24,183 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:37933 (size: 14.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,544 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1374 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,545 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,545 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.390 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,546 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,546 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,546 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,546 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,546 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,548 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,551 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,552 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.2.198.177:38433 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,553 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,553 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,554 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,556 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,575 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:37933 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,582 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.2.198.177:37664\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,618 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 63 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,618 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,622 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.075 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,622 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,622 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,624 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.478181 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,866 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,867 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,867 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,867 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,867 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,868 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,873 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 85.7 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,875 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 28.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,876 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.2.198.177:38433 (size: 28.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,877 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,877 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,877 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,879 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:25,890 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:37933 (size: 28.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,225 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 346 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,225 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,226 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.357 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,226 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,227 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,227 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,227 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,264 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,265 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,266 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,266 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,266 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,267 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,274 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 170.8 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,280 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 47.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,281 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.2.198.177:38433 (size: 47.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,282 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,282 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,282 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,284 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,296 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:37933 (size: 47.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,304 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.2.198.177:37664\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,415 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 131 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,415 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,416 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.148 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,417 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,417 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,417 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.152984 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,538 INFO codegen.CodeGenerator: Code generated in 15.394453 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,568 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,571 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,572 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,572 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,573 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,575 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,591 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 40.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,594 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,595 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.2.198.177:38433 (size: 17.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,596 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,597 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,599 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,601 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:26,617 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:37933 (size: 17.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,714 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 1114 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,714 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,715 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 1.139 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,716 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,716 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,716 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 1.147875 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,958 INFO codegen.CodeGenerator: Code generated in 61.220356 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,965 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,966 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,966 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,966 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,966 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,968 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,972 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 76.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,975 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,975 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.2.198.177:38433 (size: 24.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,976 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,977 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,977 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,980 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:27,999 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:37933 (size: 24.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,191 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 211 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,191 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,192 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.222 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,192 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,192 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,192 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,193 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,443 INFO codegen.CodeGenerator: Code generated in 133.246407 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,462 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,464 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,464 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,464 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,466 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,467 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,470 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.2 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,499 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,500 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.2.198.177:38433 (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,500 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,501 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,501 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,503 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,515 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.2.198.177:38433 in memory (size: 14.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,532 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:37933 in memory (size: 14.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,545 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:37933 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,553 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.2.198.177:37664\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,615 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.2.198.177:38433 in memory (size: 24.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,621 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 118 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,621 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,622 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:37933 in memory (size: 24.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,622 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.154 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,622 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,622 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,623 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.160259 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,694 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:37933 in memory (size: 24.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,701 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.2.198.177:38433 in memory (size: 24.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,750 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:37933 in memory (size: 17.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,770 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.2.198.177:38433 in memory (size: 17.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,837 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.2.198.177:38433 in memory (size: 28.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,848 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:37933 in memory (size: 28.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,927 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.2.198.177:38433 in memory (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,934 INFO scheduler.DAGScheduler: Registering RDD 77 (collect at AnalysisRunner.scala:326) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,935 INFO scheduler.DAGScheduler: Got map stage job 13 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,935 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 18 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,936 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,937 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:37933 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,938 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,938 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[77] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,943 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 85.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,944 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 28.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,945 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.2.198.177:38433 (size: 28.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,945 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,946 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[77] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,946 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,947 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,961 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:37933 (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,992 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.2.198.177:38433 in memory (size: 47.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:28,994 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:37933 in memory (size: 47.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,002 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.2.198.177:38433 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,003 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:37933 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,328 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 381 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,328 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,329 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (collect at AnalysisRunner.scala:326) finished in 0.389 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,329 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,329 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,329 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,329 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,359 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,360 INFO scheduler.DAGScheduler: Got job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,360 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,360 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,360 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,360 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[80] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,366 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 171.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,368 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 47.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,368 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.2.198.177:38433 (size: 47.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,369 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,369 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[80] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,369 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,371 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,381 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:37933 (size: 47.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,389 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.2.198.177:37664\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,490 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 15) in 119 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,491 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,492 INFO scheduler.DAGScheduler: ResultStage 20 (collect at AnalysisRunner.scala:326) finished in 0.130 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,492 INFO scheduler.DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,492 INFO cluster.YarnScheduler: Killing all running tasks in stage 20: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,492 INFO scheduler.DAGScheduler: Job 14 finished: collect at AnalysisRunner.scala:326, took 0.133145 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,584 INFO codegen.CodeGenerator: Code generated in 13.851703 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,614 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,615 INFO scheduler.DAGScheduler: Got job 15 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,615 INFO scheduler.DAGScheduler: Final stage: ResultStage 21 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,615 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,616 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,616 INFO scheduler.DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[90] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,623 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 40.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,624 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,625 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.2.198.177:38433 (size: 17.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,625 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,626 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[90] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,626 INFO cluster.YarnScheduler: Adding task set 21.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,627 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:29,641 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:37933 (size: 17.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,586 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 16) in 959 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,586 INFO cluster.YarnScheduler: Removed TaskSet 21.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,587 INFO scheduler.DAGScheduler: ResultStage 21 (treeReduce at KLLRunner.scala:107) finished in 0.970 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,587 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,587 INFO cluster.YarnScheduler: Killing all running tasks in stage 21: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,587 INFO scheduler.DAGScheduler: Job 15 finished: treeReduce at KLLRunner.scala:107, took 0.973071 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,771 INFO codegen.CodeGenerator: Code generated in 44.552592 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,777 INFO scheduler.DAGScheduler: Registering RDD 95 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,777 INFO scheduler.DAGScheduler: Got map stage job 16 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,777 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,778 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,778 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,778 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[95] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,783 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 76.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,785 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,785 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.2.198.177:38433 (size: 24.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,786 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,786 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[95] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,786 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,788 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:30,799 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:37933 (size: 24.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,000 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 212 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,000 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,001 INFO scheduler.DAGScheduler: ShuffleMapStage 22 (collect at AnalysisRunner.scala:326) finished in 0.221 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,001 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,001 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,001 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,001 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,044 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,045 INFO scheduler.DAGScheduler: Got job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,045 INFO scheduler.DAGScheduler: Final stage: ResultStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,045 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,046 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,046 INFO scheduler.DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[98] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,048 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 66.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,050 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,051 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.2.198.177:38433 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,051 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,052 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[98] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,052 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,053 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 18) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,062 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:37933 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,067 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.2.198.177:37664\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,076 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 18) in 23 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,076 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,077 INFO scheduler.DAGScheduler: ResultStage 24 (collect at AnalysisRunner.scala:326) finished in 0.030 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,078 INFO scheduler.DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,078 INFO cluster.YarnScheduler: Killing all running tasks in stage 24: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,079 INFO scheduler.DAGScheduler: Job 17 finished: collect at AnalysisRunner.scala:326, took 0.034328 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,202 INFO scheduler.DAGScheduler: Registering RDD 103 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,202 INFO scheduler.DAGScheduler: Got map stage job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,202 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 25 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,202 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,202 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,202 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[103] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,206 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 85.7 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,208 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 28.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,208 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.2.198.177:38433 (size: 28.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,209 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,211 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[103] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,211 INFO cluster.YarnScheduler: Adding task set 25.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,212 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,222 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:37933 (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,532 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 19) in 320 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,532 INFO cluster.YarnScheduler: Removed TaskSet 25.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,533 INFO scheduler.DAGScheduler: ShuffleMapStage 25 (collect at AnalysisRunner.scala:326) finished in 0.330 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,533 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,533 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,533 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,533 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,569 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,570 INFO scheduler.DAGScheduler: Got job 19 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,570 INFO scheduler.DAGScheduler: Final stage: ResultStage 27 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,570 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,571 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,571 INFO scheduler.DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[106] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,578 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 171.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,581 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 47.2 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,583 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.2.198.177:38433 (size: 47.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,583 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,584 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[106] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,585 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,586 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,597 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:37933 (size: 47.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,612 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.2.198.177:37664\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,721 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 20) in 135 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,721 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,722 INFO scheduler.DAGScheduler: ResultStage 27 (collect at AnalysisRunner.scala:326) finished in 0.150 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,723 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,723 INFO cluster.YarnScheduler: Killing all running tasks in stage 27: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,723 INFO scheduler.DAGScheduler: Job 19 finished: collect at AnalysisRunner.scala:326, took 0.153492 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,933 INFO codegen.CodeGenerator: Code generated in 35.848 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,987 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,988 INFO scheduler.DAGScheduler: Got job 20 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,989 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,989 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,989 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,990 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[116] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,997 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 40.5 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:31,999 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:32,000 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.2.198.177:38433 (size: 17.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:32,000 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:32,001 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[116] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:32,001 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:32,003 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:32,011 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:37933 (size: 17.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,119 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 21) in 1117 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,119 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,120 INFO scheduler.DAGScheduler: ResultStage 28 (treeReduce at KLLRunner.scala:107) finished in 1.128 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,120 INFO scheduler.DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,120 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,121 INFO scheduler.DAGScheduler: Job 20 finished: treeReduce at KLLRunner.scala:107, took 1.133983 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,245 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.2.198.177:38433 in memory (size: 28.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,266 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:37933 in memory (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,348 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.2.198.177:38433 in memory (size: 24.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,388 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:37933 in memory (size: 24.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,457 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.2.198.177:38433 in memory (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,461 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:37933 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,506 INFO codegen.CodeGenerator: Code generated in 111.871595 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,512 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,513 INFO scheduler.DAGScheduler: Got map stage job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,514 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,514 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,514 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,515 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,529 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 76.1 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,531 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,539 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.2.198.177:38433 (size: 24.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,540 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,541 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,541 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,542 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 22) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,555 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.2.198.177:38433 in memory (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,571 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:37933 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,578 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:37933 (size: 24.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,637 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.2.198.177:38433 in memory (size: 47.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,647 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:37933 in memory (size: 47.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,749 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:37933 in memory (size: 17.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,760 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.2.198.177:38433 in memory (size: 17.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,875 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.2.198.177:38433 in memory (size: 47.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,876 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:37933 in memory (size: 47.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,908 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 22) in 366 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,909 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,909 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.393 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,910 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,910 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,910 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,910 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,932 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.2.198.177:38433 in memory (size: 17.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,939 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:37933 in memory (size: 17.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,978 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:37933 in memory (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:33,980 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.2.198.177:38433 in memory (size: 28.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,051 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,055 INFO scheduler.DAGScheduler: Got job 22 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,055 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,056 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,056 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,057 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,061 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 66.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,064 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,065 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.2.198.177:38433 (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,067 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,068 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,068 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,070 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 23) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,083 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:37933 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,095 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.2.198.177:37664\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,102 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 23) in 33 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,102 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,103 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.044 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,104 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,104 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,105 INFO scheduler.DAGScheduler: Job 22 finished: collect at AnalysisRunner.scala:326, took 0.050519 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,212 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,213 INFO scheduler.DAGScheduler: Registering RDD 132 (countByKey at ColumnProfiler.scala:592) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,214 INFO scheduler.DAGScheduler: Got job 23 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,214 INFO scheduler.DAGScheduler: Final stage: ResultStage 33 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,214 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,214 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 32)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,215 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[132] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,222 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 32.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,223 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,224 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.2.198.177:38433 (size: 14.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,224 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,225 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[132] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,225 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,227 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 24) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,238 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:37933 (size: 14.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,312 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 24) in 86 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,312 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,313 INFO scheduler.DAGScheduler: ShuffleMapStage 32 (countByKey at ColumnProfiler.scala:592) finished in 0.096 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,313 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,313 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,313 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 33)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,313 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,313 INFO scheduler.DAGScheduler: Submitting ResultStage 33 (ShuffledRDD[133] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,315 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 5.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,316 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,317 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.2.198.177:38433 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,317 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,318 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (ShuffledRDD[133] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,318 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,319 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 25) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,329 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:37933 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,334 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.2.198.177:37664\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,344 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 25) in 25 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,344 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,345 INFO scheduler.DAGScheduler: ResultStage 33 (countByKey at ColumnProfiler.scala:592) finished in 0.031 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,345 INFO scheduler.DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,346 INFO cluster.YarnScheduler: Killing all running tasks in stage 33: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,346 INFO scheduler.DAGScheduler: Job 23 finished: countByKey at ColumnProfiler.scala:592, took 0.133735 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,503 INFO scheduler.DAGScheduler: Registering RDD 138 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,504 INFO scheduler.DAGScheduler: Got map stage job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,504 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 34 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,504 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,505 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,505 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 34 (MapPartitionsRDD[138] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,509 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 65.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,511 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,512 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.2.198.177:38433 (size: 23.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,512 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,513 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[138] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,513 INFO cluster.YarnScheduler: Adding task set 34.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,514 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 34.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:34,523 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:37933 (size: 23.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,090 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 34.0 (TID 26) in 576 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,090 INFO cluster.YarnScheduler: Removed TaskSet 34.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,091 INFO scheduler.DAGScheduler: ShuffleMapStage 34 (collect at AnalysisRunner.scala:326) finished in 0.584 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,091 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,091 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,091 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,091 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,120 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,121 INFO scheduler.DAGScheduler: Got job 25 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,121 INFO scheduler.DAGScheduler: Final stage: ResultStage 36 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,121 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 35)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,121 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,123 INFO scheduler.DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[141] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,128 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 117.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,129 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,131 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.2.198.177:38433 (size: 35.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,132 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,132 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[141] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,132 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,134 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 27) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,146 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:37933 (size: 35.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,154 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.2.198.177:37664\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,276 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 27) in 141 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,276 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,276 INFO scheduler.DAGScheduler: ResultStage 36 (collect at AnalysisRunner.scala:326) finished in 0.153 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,277 INFO scheduler.DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,277 INFO cluster.YarnScheduler: Killing all running tasks in stage 36: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,277 INFO scheduler.DAGScheduler: Job 25 finished: collect at AnalysisRunner.scala:326, took 0.157214 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,301 INFO codegen.CodeGenerator: Code generated in 21.741334 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,378 INFO codegen.CodeGenerator: Code generated in 12.880789 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,426 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,427 INFO scheduler.DAGScheduler: Got job 26 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,427 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,427 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,428 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,429 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[151] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,435 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 38.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,436 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,439 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.2.198.177:38433 (size: 16.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,439 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,440 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[151] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,440 INFO cluster.YarnScheduler: Adding task set 37.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,442 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 28) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:35,452 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:37933 (size: 16.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,099 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 28) in 658 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,100 INFO cluster.YarnScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,103 INFO scheduler.DAGScheduler: ResultStage 37 (treeReduce at KLLRunner.scala:107) finished in 0.673 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,105 INFO scheduler.DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,105 INFO cluster.YarnScheduler: Killing all running tasks in stage 37: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,106 INFO scheduler.DAGScheduler: Job 26 finished: treeReduce at KLLRunner.scala:107, took 0.679167 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,307 INFO codegen.CodeGenerator: Code generated in 77.101677 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,313 INFO scheduler.DAGScheduler: Registering RDD 156 (collect at AnalysisRunner.scala:326) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,313 INFO scheduler.DAGScheduler: Got map stage job 27 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,313 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,314 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,314 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,315 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,320 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 55.4 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,322 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,323 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.2.198.177:38433 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,324 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,324 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,325 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,327 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 29) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,342 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:37933 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,612 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 29) in 285 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,612 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,613 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326) finished in 0.297 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,615 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,616 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,616 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,616 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,721 INFO codegen.CodeGenerator: Code generated in 51.153759 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,732 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,733 INFO scheduler.DAGScheduler: Got job 28 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,733 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,733 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,734 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,734 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,738 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 43.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,740 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,740 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.2.198.177:38433 (size: 14.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,741 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,742 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,742 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,744 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 30) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,757 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:37933 (size: 14.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,762 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.2.198.177:37664\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,837 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 30) in 94 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,837 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,838 INFO scheduler.DAGScheduler: ResultStage 40 (collect at AnalysisRunner.scala:326) finished in 0.102 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,839 INFO scheduler.DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,840 INFO cluster.YarnScheduler: Killing all running tasks in stage 40: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,840 INFO scheduler.DAGScheduler: Job 28 finished: collect at AnalysisRunner.scala:326, took 0.108122 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:36,889 INFO codegen.CodeGenerator: Code generated in 41.392882 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,325 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,377 INFO codegen.CodeGenerator: Code generated in 9.722555 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,383 INFO scheduler.DAGScheduler: Registering RDD 164 (count at StatsGenerator.scala:66) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,383 INFO scheduler.DAGScheduler: Got map stage job 29 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,384 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 41 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,384 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,384 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,385 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[164] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,389 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 24.9 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,394 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,395 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.2.198.177:38433 (size: 11.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,397 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,397 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[164] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,398 INFO cluster.YarnScheduler: Adding task set 41.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,399 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 31) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,413 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:37933 (size: 11.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,474 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 31) in 75 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,474 INFO cluster.YarnScheduler: Removed TaskSet 41.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,475 INFO scheduler.DAGScheduler: ShuffleMapStage 41 (count at StatsGenerator.scala:66) finished in 0.089 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,475 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,476 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,476 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,477 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,504 INFO codegen.CodeGenerator: Code generated in 10.456528 ms\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,515 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,516 INFO scheduler.DAGScheduler: Got job 30 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,516 INFO scheduler.DAGScheduler: Final stage: ResultStage 43 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,517 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 42)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,517 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,517 INFO scheduler.DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[167] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,519 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 11.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,521 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,522 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.2.198.177:38433 (size: 5.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,522 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,523 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[167] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,523 INFO cluster.YarnScheduler: Adding task set 43.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,524 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 43.0 (TID 32) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,536 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-1:37933 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,540 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.2.198.177:37664\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,563 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 43.0 (TID 32) in 39 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,564 INFO cluster.YarnScheduler: Removed TaskSet 43.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,564 INFO scheduler.DAGScheduler: ResultStage 43 (count at StatsGenerator.scala:66) finished in 0.046 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,564 INFO scheduler.DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,564 INFO cluster.YarnScheduler: Killing all running tasks in stage 43: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:37,565 INFO scheduler.DAGScheduler: Job 30 finished: count at StatsGenerator.scala:66, took 0.050048 s\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,013 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:37933 in memory (size: 24.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,014 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.2.198.177:38433 in memory (size: 24.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,050 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on algo-1:37933 in memory (size: 11.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,050 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on 10.2.198.177:38433 in memory (size: 11.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,078 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.2.198.177:38433 in memory (size: 35.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,080 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:37933 in memory (size: 35.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,096 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-1:37933 in memory (size: 16.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,100 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.2.198.177:38433 in memory (size: 16.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,103 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.2.198.177:38433 in memory (size: 23.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,112 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:37933 in memory (size: 23.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,117 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.2.198.177:38433 in memory (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,125 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on algo-1:37933 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,146 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.2.198.177:38433 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,147 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:37933 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,156 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.2.198.177:38433 in memory (size: 14.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,162 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:37933 in memory (size: 14.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,168 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.2.198.177:38433 in memory (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,168 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on algo-1:37933 in memory (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,176 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.2.198.177:38433 in memory (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,178 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:37933 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,186 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.2.198.177:38433 in memory (size: 14.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,188 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on algo-1:37933 in memory (size: 14.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,432 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,446 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,478 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,479 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,487 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,508 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,583 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,584 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,593 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,599 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,655 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,655 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,655 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,698 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,699 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-7f929bf6-0c84-4b63-8a65-27638f548cf8\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,717 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-65da8436-7350-48b1-bda8-3f61bed48944\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,790 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2025-10-16 01:04:38,790 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n",
      "\n",
      "Data Quality baseline job complete.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor, DatasetFormat\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Build baseline from scaled training data. Use the updated csv file. \n",
    "train_s3_path = f's3://{bucket}/nutriscore-prediction-xgboost/train/train_scaled_ready_final.csv'\n",
    "\n",
    "# S3 path for data quality reports\n",
    "data_quality_report_path = f\"s3://{bucket}/nutriscore-prediction-xgboost/data-quality-reports\"\n",
    "\n",
    "# Create Data Quality Monitor\n",
    "data_quality_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    base_job_name='nutriscore-data-quality-baseline',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "data_quality_baseline_job_name = f\"nutriscore-data-quality-baseline-job-{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "\n",
    "print(\"Starting Data Quality baseline suggestion job...\")\n",
    "\n",
    "# Run baseline job\n",
    "data_quality_monitor.suggest_baseline(\n",
    "    baseline_dataset=train_s3_path,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=data_quality_report_path,\n",
    "    wait=True,\n",
    "    job_name=data_quality_baseline_job_name\n",
    ")\n",
    "\n",
    "print(\"\\nData Quality baseline job complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b48b8562-335f-4315-98da-6f1f19736294",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T21:01:40.145588Z",
     "iopub.status.busy": "2025-10-15T21:01:40.145138Z",
     "iopub.status.idle": "2025-10-15T21:01:40.258002Z",
     "shell.execute_reply": "2025-10-15T21:01:40.257123Z",
     "shell.execute_reply.started": "2025-10-15T21:01:40.145564Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingInputs': [{'InputName': 'baseline_dataset_input',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-381492037991/nutriscore-prediction-xgboost/train/train_scaled_ready_final.csv',\n",
       "    'LocalPath': '/opt/ml/processing/input/baseline_dataset_input',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}}],\n",
       " 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-381492037991/nutriscore-prediction-xgboost/data-quality-reports',\n",
       "     'LocalPath': '/opt/ml/processing/output',\n",
       "     'S3UploadMode': 'EndOfJob'},\n",
       "    'AppManaged': False}]},\n",
       " 'ProcessingJobName': 'nutriscore-data-quality-baseline-job-2025-10-15-20-53-27',\n",
       " 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1,\n",
       "   'InstanceType': 'ml.m5.xlarge',\n",
       "   'VolumeSizeInGB': 20}},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 3600},\n",
       " 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer'},\n",
       " 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}',\n",
       "  'dataset_source': '/opt/ml/processing/input/baseline_dataset_input',\n",
       "  'output_path': '/opt/ml/processing/output',\n",
       "  'publish_cloudwatch_metrics': 'Disabled'},\n",
       " 'RoleArn': 'arn:aws:iam::381492037991:role/LabRole',\n",
       " 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:381492037991:processing-job/nutriscore-data-quality-baseline-job-2025-10-15-20-53-27',\n",
       " 'ProcessingJobStatus': 'Completed',\n",
       " 'ExitMessage': 'Completed: Job completed successfully with no violations.',\n",
       " 'ProcessingEndTime': datetime.datetime(2025, 10, 15, 20, 57, 18, tzinfo=tzlocal()),\n",
       " 'ProcessingStartTime': datetime.datetime(2025, 10, 15, 20, 54, 3, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2025, 10, 15, 20, 58, 29, 170000, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2025, 10, 15, 20, 53, 27, 318000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '7a17db0d-f3c1-41b4-b1d5-2958b9e3a842',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '7a17db0d-f3c1-41b4-b1d5-2958b9e3a842',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1881',\n",
       "   'date': 'Wed, 15 Oct 2025 21:01:40 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_quality_monitor.latest_baselining_job.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92b593f1-f4d3-4dd3-bf7b-fb216ccd9d10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T00:21:39.407893Z",
     "iopub.status.busy": "2025-10-16T00:21:39.407617Z",
     "iopub.status.idle": "2025-10-16T00:21:41.327247Z",
     "shell.execute_reply": "2025-10-16T00:21:41.326155Z",
     "shell.execute_reply.started": "2025-10-16T00:21:39.407873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Data Quality monitoring schedule: nutriscore-data-quality-schedule-2025-10-16-00-21-39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: nutriscore-data-quality-schedule-2025-10-16-00-21-39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data Quality monitoring schedule created successfully.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "data_quality_schedule_name = f\"nutriscore-data-quality-schedule-{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "\n",
    "statistics_file = (\n",
    "    \"s3://sagemaker-us-east-1-381492037991/\"\n",
    "    \"nutriscore-prediction-xgboost/data-quality-reports/statistics.json\" # You can copy this path from your s3. \n",
    ")\n",
    "constraints_file = (\n",
    "    \"s3://sagemaker-us-east-1-381492037991/\"\n",
    "    \"nutriscore-prediction-xgboost/data-quality-reports/constraints.json\"\n",
    ")\n",
    "\n",
    "print(f\"Creating Data Quality monitoring schedule: {data_quality_schedule_name}\")\n",
    "\n",
    "data_quality_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=data_quality_schedule_name,\n",
    "    endpoint_input=\"xgb-nutriscore-model-quality-monitor-2025-10-15-07-33-09\",  # Replace with your endpoint name from the code above \n",
    "    output_s3_uri=data_quality_report_path,\n",
    "    statistics=statistics_file,\n",
    "    constraints=constraints_file,\n",
    "    schedule_cron_expression=\"cron(0 * ? * * *)\"  #  Runs every hour\n",
    ")\n",
    "\n",
    "print(\" Data Quality monitoring schedule created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb091d13-a75d-462a-ba94-1fd5ffe2b34e",
   "metadata": {},
   "source": [
    "## Generate Baseline for Model Quality Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64294d-af15-41b3-8d79-965a2b868eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a8e3d-56db-4ca7-9e11-8d96808ba245",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.374404Z",
     "iopub.status.idle": "2025-10-15T07:23:07.374665Z",
     "shell.execute_reply": "2025-10-15T07:23:07.374559Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.374548Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get validation dataset\n",
    "# get from previous notebook\n",
    "val_s3_path = 's3://sagemaker-us-east-1-654654380268/nutriscore-prediction-xgboost/validation/val_scaled.csv' \n",
    "!aws s3 cp {val_s3_path} ./\n",
    "val_local_path = './val_scaled.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61590459-9dee-4340-8f81-22bdfb76d1c6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.377971Z",
     "iopub.status.idle": "2025-10-15T07:23:07.378246Z",
     "shell.execute_reply": "2025-10-15T07:23:07.378137Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.378126Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your validation dataset should h first column\n",
    "limit = 500 # number of samples for baseline\n",
    "baseline_file_name = 'val_pred_baseline.csv'\n",
    "i = 0\n",
    "\n",
    "# Create a new file for your baseline data\n",
    "with open(f\"{baseline_file_name}\", \"w\") as baseline_file:\n",
    "    # Header for a regression baseline\n",
    "    baseline_file.write(\"prediction,label\\n\")\n",
    "    \n",
    "    # Open validation data file\n",
    "    with open(val_local_path, \"r\") as f:\n",
    "        for row in f:\n",
    "            # With true score in first column\n",
    "            (label, input_cols) = row.split(\",\", 1)\n",
    "            \n",
    "            # Get the predicted score from the endpoint\n",
    "            predicted_score = float(predictor.predict(input_cols))\n",
    "            \n",
    "            # Write the predicted score and the true label to the baseline file\n",
    "            baseline_file.write(f\"{predicted_score},{label.strip()}\\n\")\n",
    "            \n",
    "            i += 1\n",
    "            if i >= limit:\n",
    "                break\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            sleep(0.5)\n",
    "print()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a2dc9-b709-43c4-95fb-9de38a027707",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.378842Z",
     "iopub.status.idle": "2025-10-15T07:23:07.379091Z",
     "shell.execute_reply": "2025-10-15T07:23:07.378987Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.378976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Examine predictions from model\n",
    "!head {baseline_file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b2ac21-01e0-443a-bbc3-dd35b9667fb4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.379781Z",
     "iopub.status.idle": "2025-10-15T07:23:07.380203Z",
     "shell.execute_reply": "2025-10-15T07:23:07.380039Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.380023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Upload predictions as baseline dataset\n",
    "baseline_prefix = prefix + \"/baselining\"\n",
    "baseline_data_prefix = baseline_prefix + \"/data\"\n",
    "baseline_results_prefix = baseline_prefix + \"/results\"\n",
    "\n",
    "baseline_data_uri = f\"s3://{bucket}/{baseline_data_prefix}\"\n",
    "baseline_results_uri = f\"s3://{bucket}/{baseline_results_prefix}\"\n",
    "print(f\"Baseline data uri: {baseline_data_uri}\")\n",
    "print(f\"Baseline results uri: {baseline_results_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52022c01-d2e3-427c-8e31-955a7b676252",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.380786Z",
     "iopub.status.idle": "2025-10-15T07:23:07.381125Z",
     "shell.execute_reply": "2025-10-15T07:23:07.380968Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.380954Z"
    }
   },
   "outputs": [],
   "source": [
    "# Upload baseline dataset\n",
    "baseline_dataset_uri = S3Uploader.upload(f\"{baseline_file_name}\", baseline_data_uri)\n",
    "baseline_dataset_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c1d46-fdd3-4f8b-a338-7d31c795d4a9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.381769Z",
     "iopub.status.idle": "2025-10-15T07:23:07.382105Z",
     "shell.execute_reply": "2025-10-15T07:23:07.381946Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.381931Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the model quality monitoring object\n",
    "nutriscore_model_quality_monitor = ModelQualityMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26394e30-5b24-4af8-8e05-501729294b1b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.382612Z",
     "iopub.status.idle": "2025-10-15T07:23:07.383005Z",
     "shell.execute_reply": "2025-10-15T07:23:07.382844Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.382828Z"
    }
   },
   "outputs": [],
   "source": [
    "# Name of the model quality baseline job\n",
    "baseline_job_name = f\"xgb-nutriscore-model-baseline-job-{datetime.now():%Y-%m-%d-%H%M}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02d748-8efa-4355-a366-cca8a6a2638a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.383568Z",
     "iopub.status.idle": "2025-10-15T07:23:07.383913Z",
     "shell.execute_reply": "2025-10-15T07:23:07.383750Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.383735Z"
    }
   },
   "outputs": [],
   "source": [
    "# Execute the baseline suggestion job\n",
    "job = nutriscore_model_quality_monitor.suggest_baseline(\n",
    "    job_name=baseline_job_name,\n",
    "    baseline_dataset=baseline_dataset_uri,\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    problem_type=\"Regression\",\n",
    "    inference_attribute=\"prediction\", # model output\n",
    "    ground_truth_attribute=\"label\", # true score\n",
    ")\n",
    "job.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201254ab-fe46-469d-9a92-0202d9c55b1a",
   "metadata": {},
   "source": [
    "## Explore Results of Baseline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe61539-f02f-4a28-91a0-e13c4d7b70b0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.384963Z",
     "iopub.status.idle": "2025-10-15T07:23:07.385295Z",
     "shell.execute_reply": "2025-10-15T07:23:07.385138Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.385123Z"
    }
   },
   "outputs": [],
   "source": [
    "baseline_job = nutriscore_model_quality_monitor.latest_baselining_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6698f6a-498f-4d90-807a-ba2ac4c44280",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.386289Z",
     "iopub.status.idle": "2025-10-15T07:23:07.386628Z",
     "shell.execute_reply": "2025-10-15T07:23:07.386471Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.386456Z"
    }
   },
   "outputs": [],
   "source": [
    "# View metrics\n",
    "binary_metrics = baseline_job.baseline_statistics().body_dict[\"regression_metrics\"]\n",
    "pd.json_normalize(binary_metrics).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1158ee-28f8-46e8-919d-a6f75b98c625",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.387990Z",
     "iopub.status.idle": "2025-10-15T07:23:07.388339Z",
     "shell.execute_reply": "2025-10-15T07:23:07.388180Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.388165Z"
    }
   },
   "outputs": [],
   "source": [
    "# View constraints\n",
    "regression_constraints = pd.DataFrame(baseline_job.suggested_constraints().body_dict[\"regression_constraints\"]).T\n",
    "regression_constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735fe903-7e57-4a9d-b869-174ef5257f17",
   "metadata": {},
   "source": [
    "## Setup Continuous Model Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ca1646-2bc2-4962-88c2-fadc5d077613",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.389126Z",
     "iopub.status.idle": "2025-10-15T07:23:07.389492Z",
     "shell.execute_reply": "2025-10-15T07:23:07.389334Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.389319Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get some samples from scaled production data split\n",
    "# From notebook 04\n",
    "prod_scaled_path = 's3://sagemaker-us-east-1-654654380268/nutriscore-prediction-xgboost/prod/prod_scaled.csv'\n",
    "prod_scaled_df = pd.read_csv(prod_scaled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e1ad1-da9b-4e76-ae15-8c4b6bcc9b11",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.390189Z",
     "iopub.status.idle": "2025-10-15T07:23:07.390526Z",
     "shell.execute_reply": "2025-10-15T07:23:07.390367Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.390352Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a sample for the test run (500 random rows)\n",
    "sample_traffic_df = prod_scaled_df.sample(n=500)\n",
    "print(f\"Using a sample of {len(sample_traffic_df)} rows from the production dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f1078-f8a7-49ab-8d8c-f208fcaa06b7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.391253Z",
     "iopub.status.idle": "2025-10-15T07:23:07.391653Z",
     "shell.execute_reply": "2025-10-15T07:23:07.391491Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.391475Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate prediction data on sample data\n",
    "\n",
    "# Store ground truth with inference ids\n",
    "ground_truth_labels_with_ids = []\n",
    "\n",
    "# First column is the label (true score) and the rest are features\n",
    "label_column = sample_traffic_df.columns[0]\n",
    "feature_columns = sample_traffic_df.columns[1:]\n",
    "\n",
    "print(\"Sending pre-scaled production samples for inference...\")\n",
    "\n",
    "# Send samples for inference\n",
    "for index, row in tqdm(sample_traffic_df.iterrows(), total=sample_traffic_df.shape[0]):\n",
    "    features_payload = \",\".join(map(str, row[feature_columns].values))\n",
    "    \n",
    "    # Invoke the endpoint\n",
    "    sagemaker_session.sagemaker_runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"text/csv\",\n",
    "        Body=features_payload,\n",
    "        InferenceId=str(index), # use index as unique id\n",
    "    )\n",
    "    \n",
    "    # Store the true label and its corresponding ID for the ground truth upload\n",
    "    ground_truth_labels_with_ids.append({\n",
    "        \"inference_id\": index,\n",
    "        \"label\": row[label_column]\n",
    "    })\n",
    "    sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee5e45-f474-40b0-b84d-d436cb5259eb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.392337Z",
     "iopub.status.idle": "2025-10-15T07:23:07.392677Z",
     "shell.execute_reply": "2025-10-15T07:23:07.392520Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.392505Z"
    }
   },
   "outputs": [],
   "source": [
    "# View captured data\n",
    "print(\"Waiting for captures to show up\", end=\"\")\n",
    "for _ in range(120):\n",
    "    capture_files = sorted(S3Downloader.list(f\"{s3_capture_upload_path}/{endpoint_name}\"))\n",
    "    if capture_files:\n",
    "        capture_file = S3Downloader.read_file(capture_files[-1]).split(\"\\n\")\n",
    "        capture_record = json.loads(capture_file[0])\n",
    "        if \"inferenceId\" in capture_record[\"eventMetadata\"]:\n",
    "            break\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    sleep(1)\n",
    "print()\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files[-3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a48b4-4154-4d6c-ae72-7d21f4a85861",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.393783Z",
     "iopub.status.idle": "2025-10-15T07:23:07.394129Z",
     "shell.execute_reply": "2025-10-15T07:23:07.393969Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.393953Z"
    }
   },
   "outputs": [],
   "source": [
    "# View single capture\n",
    "print(json.dumps(capture_record, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d707a-f912-4e24-8515-78655d83b3f0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.395060Z",
     "iopub.status.idle": "2025-10-15T07:23:07.395401Z",
     "shell.execute_reply": "2025-10-15T07:23:07.395245Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.395230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Format and upload the true labels\n",
    "ground_truth_records = []\n",
    "for item in ground_truth_labels_with_ids:\n",
    "    record = {\n",
    "        \"groundTruthData\": {\n",
    "            \"data\": str(item['label']), # The true nutrition score\n",
    "            \"encoding\": \"CSV\",\n",
    "        },\n",
    "        \"eventMetadata\": {\n",
    "            \"eventId\": item['inference_id'],\n",
    "        },\n",
    "        \"eventVersion\": \"0\",\n",
    "    }\n",
    "    ground_truth_records.append(json.dumps(record))\n",
    "\n",
    "# Convert the list of JSON strings into a single string with newlines\n",
    "ground_truth_data_to_upload = \"\\n\".join(ground_truth_records)\n",
    "\n",
    "# Upload to the S3 path the monitor is watching\n",
    "target_s3_uri = f\"{ground_truth_upload_path}/{datetime.utcnow():%Y/%m/%d/%H}/ground_truth.jsonl\"\n",
    "\n",
    "print(f\"\\nUploading {len(ground_truth_records)} ground truth records to {target_s3_uri}\")\n",
    "S3Uploader.upload_string_as_file_body(ground_truth_data_to_upload, target_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bd5cd0-9aec-482b-b5f8-67ca0cfa5886",
   "metadata": {},
   "source": [
    "## Create Monitoring Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d5e6a-9ea5-4371-b6a7-bd3c275e7bea",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.396257Z",
     "iopub.status.idle": "2025-10-15T07:23:07.396595Z",
     "shell.execute_reply": "2025-10-15T07:23:07.396439Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.396425Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set monitor schedule name\n",
    "nutriscore_monitor_schedule_name = f\"nutriscore-monitoring-schedule-{datetime.now():%Y-%m-%d-%H-%M-%S}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5a663-d3d7-45ef-aad3-850b97e8ad81",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.397309Z",
     "iopub.status.idle": "2025-10-15T07:23:07.397685Z",
     "shell.execute_reply": "2025-10-15T07:23:07.397486Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.397471Z"
    }
   },
   "outputs": [],
   "source": [
    "# EndpointInput for regression\n",
    "endpointInput = EndpointInput(\n",
    "    endpoint_name=predictor.endpoint_name,\n",
    "    inference_attribute=\"0\", # first column contains inference\n",
    "    destination=\"/opt/ml/processing/input_data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d113b9-f341-4629-acc9-6275555ca8ef",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.398400Z",
     "iopub.status.idle": "2025-10-15T07:23:07.398734Z",
     "shell.execute_reply": "2025-10-15T07:23:07.398580Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.398565Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the monitoring schedule to execute every hour\n",
    "response = nutriscore_model_quality_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=nutriscore_monitor_schedule_name,\n",
    "    endpoint_input=endpointInput,\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    problem_type=\"Regression\",\n",
    "    ground_truth_input=ground_truth_upload_path,\n",
    "    constraints=baseline_job.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd702de8-579f-49f1-9f25-16559b905cf9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.400299Z",
     "iopub.status.idle": "2025-10-15T07:23:07.400650Z",
     "shell.execute_reply": "2025-10-15T07:23:07.400491Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.400475Z"
    }
   },
   "outputs": [],
   "source": [
    "# Examine schedule on monitor\n",
    "nutriscore_model_quality_monitor.describe_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f160a-6195-453b-a8ba-e4efbb46e8d7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.401451Z",
     "iopub.status.idle": "2025-10-15T07:23:07.401803Z",
     "shell.execute_reply": "2025-10-15T07:23:07.401642Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.401626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initially there will be no executions since the first execution happens at the top of the hour\n",
    "executions = nutriscore_model_quality_monitor.list_executions()\n",
    "executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f5b616-622c-40b1-a7d2-02c81a78dbc5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.402575Z",
     "iopub.status.idle": "2025-10-15T07:23:07.402920Z",
     "shell.execute_reply": "2025-10-15T07:23:07.402762Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.402747Z"
    }
   },
   "outputs": [],
   "source": [
    "# Wait for the first execution of the monitoring_schedule\n",
    "print(\"Waiting for first execution\", end=\"\")\n",
    "while True:\n",
    "    execution = nutriscore_model_quality_monitor.describe_schedule().get(\n",
    "        \"LastMonitoringExecutionSummary\"\n",
    "    )\n",
    "    if execution:\n",
    "        break\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    sleep(10)\n",
    "print()\n",
    "print(\"Execution found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c44c2ee-5780-47bb-b2a5-31535aef3349",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.403709Z",
     "iopub.status.idle": "2025-10-15T07:23:07.404055Z",
     "shell.execute_reply": "2025-10-15T07:23:07.403895Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.403880Z"
    }
   },
   "outputs": [],
   "source": [
    "# View execution details\n",
    "while not executions:\n",
    "    executions = nutriscore_model_quality_monitor.list_executions()\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    sleep(10)\n",
    "latest_execution = executions[-1]\n",
    "latest_execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef051c3a-f114-407b-ac04-46846fe97568",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.404751Z",
     "iopub.status.idle": "2025-10-15T07:23:07.405091Z",
     "shell.execute_reply": "2025-10-15T07:23:07.404931Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.404916Z"
    }
   },
   "outputs": [],
   "source": [
    "# View execution status\n",
    "status = execution[\"MonitoringExecutionStatus\"]\n",
    "\n",
    "while status in [\"Pending\", \"InProgress\"]:\n",
    "    print(\"Waiting for execution to finish\", end=\"\")\n",
    "    latest_execution.wait(logs=False)\n",
    "    latest_job = latest_execution.describe()\n",
    "    print()\n",
    "    print(f\"{latest_job['ProcessingJobName']} job status:\", latest_job[\"ProcessingJobStatus\"])\n",
    "    print(\n",
    "        f\"{latest_job['ProcessingJobName']} job exit message, if any:\",\n",
    "        latest_job.get(\"ExitMessage\"),\n",
    "    )\n",
    "    print(\n",
    "        f\"{latest_job['ProcessingJobName']} job failure reason, if any:\",\n",
    "        latest_job.get(\"FailureReason\"),\n",
    "    )\n",
    "    sleep(\n",
    "        30\n",
    "    )  # model quality executions consist of two Processing jobs, wait for second job to start\n",
    "    latest_execution = nutriscore_model_quality_monitor.list_executions()[-1]\n",
    "    execution = nutriscore_model_quality_monitor.describe_schedule()[\"LastMonitoringExecutionSummary\"]\n",
    "    status = execution[\"MonitoringExecutionStatus\"]\n",
    "\n",
    "print(\"Execution status is:\", status)\n",
    "\n",
    "if status != \"Completed\":\n",
    "    print(execution)\n",
    "    print(\n",
    "        \"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9960ca8-cc7b-4f67-a579-e35807c5bf34",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.406066Z",
     "iopub.status.idle": "2025-10-15T07:23:07.406410Z",
     "shell.execute_reply": "2025-10-15T07:23:07.406251Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.406236Z"
    }
   },
   "outputs": [],
   "source": [
    "# View generated report uri\n",
    "latest_execution = nutriscore_model_quality_monitor.list_executions()[-1]\n",
    "report_uri = latest_execution.describe()[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\n",
    "    \"S3Uri\"\n",
    "]\n",
    "print(\"Report Uri:\", report_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9d5e3-a4ec-4b99-afaa-721e7c8d1ba0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.407229Z",
     "iopub.status.idle": "2025-10-15T07:23:07.407561Z",
     "shell.execute_reply": "2025-10-15T07:23:07.407403Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.407387Z"
    }
   },
   "outputs": [],
   "source": [
    "# View violations generated by monitoring schedule\n",
    "pd.options.display.max_colwidth = None\n",
    "violations = latest_execution.constraint_violations().body_dict[\"violations\"]\n",
    "violations_df = pd.json_normalize(violations)\n",
    "violations_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49247b3f-a8e2-42e8-89fd-634f25afb410",
   "metadata": {},
   "source": [
    "## Create Quality CloudWatch Alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446f0ef2-0300-4a2e-b81f-b473eddf08bd",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.408491Z",
     "iopub.status.idle": "2025-10-15T07:23:07.408839Z",
     "shell.execute_reply": "2025-10-15T07:23:07.408679Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.408664Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get thresholds from baseline constraints\n",
    "rmse_threshold = regression_constraints.loc['rmse', 'threshold']\n",
    "rmse_operator = regression_constraints.loc['rmse', 'comparison_operator']\n",
    "print(f\"RMSE Threshold from baseline constraints: {rmse_threshold}\")\n",
    "print(f\"RMSE Comparison Operator from baseline constraints: {rmse_operator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45690066-ddda-4be2-a852-92245903862b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.409694Z",
     "iopub.status.idle": "2025-10-15T07:23:07.410048Z",
     "shell.execute_reply": "2025-10-15T07:23:07.409883Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.409867Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a CloudWatch Alarm for a regression metrics\n",
    "print(\"Creating CloudWatch alarm for RMSE...\")\n",
    "alarm_name = \"NUTRISCORE_MODEL_RMSE_DRIFT\"\n",
    "alarm_desc = \"Trigger an alarm when the model's RMSE exceeds the baseline threshold.\"\n",
    "cw_quality_dimensions = [\n",
    "    {\"Name\": \"Endpoint\", \"Value\": endpoint_name},\n",
    "    {\"Name\": \"MonitoringSchedule\", \"Value\": nutriscore_monitor_schedule_name},\n",
    "]\n",
    "\n",
    "cw_client.put_metric_alarm(\n",
    "    AlarmName=alarm_name,\n",
    "    AlarmDescription=alarm_desc,\n",
    "    ActionsEnabled=True,\n",
    "    MetricName=\"rmse\",\n",
    "    Namespace=namespace,\n",
    "    Statistic=\"Average\",\n",
    "    Dimensions=cw_quality_dimensions,\n",
    "    Period=3600, # Checks every hour\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=rmse_threshold,\n",
    "    ComparisonOperator=rmse_operator,\n",
    "    TreatMissingData=\"breaching\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d049b2-c896-4ab9-9799-89e66fc64d8a",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb476b9-1bf2-43af-b7f1-435e711fe03d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T07:23:07.410979Z",
     "iopub.status.idle": "2025-10-15T07:23:07.411331Z",
     "shell.execute_reply": "2025-10-15T07:23:07.411164Z",
     "shell.execute_reply.started": "2025-10-15T07:23:07.411149Z"
    }
   },
   "outputs": [],
   "source": [
    "# Delete monitoring schedules\n",
    "nutriscore_model_quality_monitor.delete_monitoring_schedule()\n",
    "data_quality_monitor.delete_monitoring_schedule()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
